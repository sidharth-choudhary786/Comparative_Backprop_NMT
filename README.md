# Neural Machine Translation: Optimizer Analysis

## Research Overview
**Exploring optimizer efficacy for low-resource language pairs** - 
Comprehensive comparison of SGD, Adam, RMSProp, Adagrad optimizers 
for English-Hindi translation using Seq2Seq model with attention mechanism.

*Motivated by the scarcity of optimized NMT solutions for Indian languages, this project demonstrates that classical optimizers like RMSProp can outperform Adam for specific low-resource translation tasks.*

## Performance Results

### Optimizer Comparison
| Algorithm | BLEU Score | Performance | Train Loss | Val Loss |
|-----------|------------|-------------|------------|----------|
| RMSProp   | 17.63      | Best        | 28.80      | 49.90    |
| Adagrad   | 13.73      | Excellent   | 30.38      | 52.24    |
| Adam      | 9.79       | Good        | 43.03      | 57.52    |
| SGD       | 5.76       | Basic       | 52.16      | 63.03    |

### Key Achievements
- **206% improvement** from SGD to RMSProp
- **17.61 BLEU score** achieved with custom model
- **15-minute training** time on CPU
- **1000 parallel sentences** dataset

### Dataset Information (ACCURATE)
- **Total Samples Used**: 1,000 sentence pairs
- **Training Set**: 900 sentences (90%)
- **Validation Set**: 100 sentences (10%)
- **Dataset Source**: Curated parallel corpus subset
- **Purpose**: Efficient optimizer comparison with representative data

  
### Technical Implementation

#### Model Architecture
English ‚Üí Tokenization ‚Üí Encoder ‚Üí Attention ‚Üí Decoder ‚Üí Hindi

#### Implementation Challenges & Solutions
- **Vocabulary Size**: Limited to 4000 subwords balancing Hindi morphology complexity and model size
- **Batch Size Constraint**: 16 maximum due to attention mechanism memory requirements on 16GB GPU(not fixed)
- **Convergence Strategy**: Early stopping at 4 epochs based on validation loss plateau observation
- **Dataset Quality**: Curated 1,000 parallel sentences ensuring grammatical correctness
- 
#### Model Training Details
- **Early Stopping**: Triggered when validation loss didn't improve for 2 epochs
- **Gradient Clipping**: Applied at 1.0 to prevent explosion
- **Learning Rate**: Manually tuned for each optimizer's characteristics


### Specifications
- Embedding: 256 dimensions
- Hidden Size: 256 units  
- Vocabulary: 4000 subwords
- Batch Size: 16
- Epochs: 4


### Usage Example
```python
# Sample inference code from our trained model
from model import load_checkpoint, translate_text

# Load the best performing model (RMSProp trained)
model = load_checkpoint('checkpoints/rmsprop_best.pth')
result = translate_text("Hello, how are you?")
print(result)  # Output: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?
```



## Installation & Usage
## Quick Start
```bash
pip install -r requirements.txt
python comparative_mnt.py
```
### Common Issues & Solutions
- **CUDA out of memory**: Reduce batch_size to 8
- **SentencePiece installation failed**: Use `pip install sentencepiece==0.1.99`
- **Model loading error**: Check checkpoint path in config

### Training Insights
*Based on experimental observations:*
- **RMSProp** showed fastest convergence with stable gradient updates
- **Adagrad** performed surprisingly well despite being less popular for NMT
- **Adam** underperformed potentially due to adaptive learning rate mismatch
- **SGD** required significantly more epochs to achieve comparable results

### Research Insights
- **RMSProp's consistency** makes it ideal for sequence tasks
- **Adam's adaptive learning** sometimes hinders convergence in low-resource scenarios
- **Classical optimizers** deserve reconsideration in specific NLP tasks


### Training Results Summary
RMSProp: BLEU = 17.61  Best
Adagrad: BLEU = 13.71  Excellent 
Adam:    BLEU = 9.79   Good
SGD:     BLEU = 5.76   Basic



---

# Business Applications

##  Real-world Use Cases 
- **E-commerce localization** - Translate product descriptions
- **Multilingual chatbots** - Customer support in Hindi/English
- **Educational content** - Study materials translation
- **Healthcare translation** - Medical reports and prescriptions

### Industry Impact 
- **206% improvement** over baseline SGD
- **15-minute training** suitable for rapid prototyping
- **17.61 BLEU score** demonstrating optimizer significance



---


## Technical Details

### Experimental Setup
- **Dataset Size**: 1,000 carefully selected parallel sentences
- **Split**: 900 training + 100 validation pairs
- **Sample Quality**: Manually verified for translation accuracy
- **Scope**: Academic-scale demonstration of optimizer efficacy

### Infrastructure
**Framework**: PyTorch 2.0.1  
**Tokenization**: SentencePiece (4000 subwords)  
**Evaluation**: BLEU score + Human Evaluation  
**Visualization**: Matplotlib + Seaborn  
**Hardware**: GPU NVIDIA T4 (16GB)  

### Visualization
BLEU Score Comparison
![BLEU Score Comparison](Training.png)

### Model Architecture 
English ‚Üí Embedding(256) ‚Üí LSTM Encoder ‚Üí Attention ‚Üí LSTM Decoder ‚Üí Hindi Output


---

## Project Structure

```
comparative_mmt.py          # Main training script
utils/                      # Utility functions
‚îú‚îÄ‚îÄ data_loader.py          # Data preprocessing
‚îú‚îÄ‚îÄ eval_metrics.py         # BLEU score calculation
‚îî‚îÄ‚îÄ visualization.py        # Graphs and charts
models/                     # Model architectures
‚îú‚îÄ‚îÄ seq2seq_attention.py
‚îî‚îÄ‚îÄ optimizer_configs.py
configs/                    # Training configurations
‚îî‚îÄ‚îÄ params.yaml
```


## Developer

### Project Journey
**Sidharth Choudhary** 
*"This project emerged from practical challenges in deploying NMT for Hindi content. While most research focuses on high-resource languages, we demonstrated that optimizer selection significantly impacts low-resource scenarios. The 372% improvement from SGD to RMSProp highlights the importance of optimizer tuning beyond default choices."*

### Connect & Collaborate
üìß jattsidh786@gmail.com  
üîó https://www.linkedin.com/in/sidharth-choudhary786  
üêô https://github.com/sidharth-choudhary786


















